# AI 활용 워크플로우

> 레거시 Spring Boot 프로젝트에 인수 테스트를 도입하기 위해 Claude Code와 ChatGPT를 활용한 과정을 기록한다.

---

## 전체 흐름 요약

```
[1] 환경 구성 ──▶ [2-3] 프로젝트 분석 ──▶ [4-5] 스킬·규칙 정의 ──▶ [6-12] 반복 작성 ──▶ [13] 전략 문서화
    Claude           Claude + GPT          Claude                   Claude + GPT          Claude
```

| 단계 | 목적 | 사용 도구 |
|------|------|-----------|
| 1 | 프로젝트 규칙 초기화 | Claude Code |
| 2-3 | 프로젝트 분석 + 프롬프트 설계 | Claude Code → ChatGPT |
| 4-5 | 재사용 가능한 스킬 생성 | Claude Code |
| 6-12 | 테스트 코드 반복 작성·개선 | Claude Code + ChatGPT |
| 13 | 전략 문서화 | Claude Code |

---

## 단계별 상세

### 1단계: CLAUDE.md 초기화

```bash
claude /init
```

`/init` 명령으로 `CLAUDE.md`를 생성했다. 이 파일은 Claude Code가 프로젝트 컨텍스트를 이해하는 기반이 된다. 빌드 명령, 아키텍처 개요, 코드 컨벤션 등 프로젝트의 기본 규칙을 정의한다.

**핵심:** CLAUDE.md는 매 대화마다 자동으로 로드된다. 여기에 규칙을 명시하면 반복 지시 없이도 일관된 결과를 얻을 수 있다.

---

### 2단계: 단순 프롬프트로 프로젝트 분석 (첫 시도)

**프롬프트:**
> API 단위로 핵심 기능을 알려줘. 각 기능을 상세하게 설명해줘.

**결과:** Claude가 API별로 설명을 제공했지만, **의도가 아닌 구현 코드 기준**으로 분석했다. 예를 들어 "이 메서드는 Repository를 호출한다"는 식의 내부 구현 설명이 주를 이루었다.

**교훈:** "분석해줘"라는 단순 프롬프트는 방향이 없다. AI는 코드를 있는 그대로 설명하지, 우리가 원하는 관점으로 해석하지 않는다. **관점을 명시해야 한다.**

---

### 3단계: GPT로 프롬프트 설계

2단계의 한계를 느끼고, ChatGPT에게 **프롬프트 자체를 설계**해달라고 요청했다.

**GPT에게 한 요청:**
> 레거시 프로젝트를 분석하려고 해. 프롬프트를 어떤 식으로 짜야 할까? 단순히 "프로젝트를 분석해줘"가 아니라, 내가 전문가라면 어떻게 분석해달라고 하는 게 좋을까? 결론적으로는 레거시 코드의 인수 테스트 코드를 작성하기 위한 분석이야. 사용자 관점에서 행위를 검증하는 테스트를 작성해야 돼.

**GPT의 역할:** 메타 프롬프팅 — "무엇을 물어볼지"를 설계하는 단계. GPT가 범용적인 프롬프트 템플릿을 만들어줬다. 이 템플릿에는 다음과 같은 관점이 포함되었다:

- 역할 부여: "너는 레거시 시스템의 인수 테스트 설계 전문가야"
- 목표 명시: "사용자 관점에서 행위를 검증하는 자동화 테스트"
- 검증 기준: "내부 구현이 아니라 관찰 가능한 결과 기준"
- 작업 절차: 시나리오 도출 → Gherkin → 우선순위 → 코드

**교훈:** AI끼리 역할을 분리하면 효과적이다. GPT는 "어떻게 물어볼지"를, Claude는 "실제로 실행하기"를 담당했다.

---

### 4단계: Claude Code 스킬 생성

GPT가 만들어준 프롬프트 템플릿을 Claude에게 넘기고, **프로젝트에 맞는 스킬로 변환**해달라고 요청했다. 이후 [Claude Code 스킬 공식 문서](https://code.claude.com/docs/ko/skills)를 참고하여 Skills 표준 형식으로 다듬었다.

**프롬프트:**
> skill을 만들려고 해. 이런 범용적인 프롬프트가 있는데, 프로젝트를 분석해서 너에게 맞는 인수 테스트용 스킬을 만들어줘.

Claude가 프로젝트의 엔드포인트, 엔티티 관계, 비즈니스 규칙을 분석한 뒤, 공식 문서의 Skills 디렉토리 구조에 따라 `.claude/skills/acceptance-test/SKILL.md`로 스킬 파일을 생성했다.

**공식 문서 기반 적용 사항:**

| 항목 | 설명 |
|------|------|
| **Skills 디렉토리 구조** | `.claude/commands/` (레거시) 대신 `.claude/skills/<skill-name>/SKILL.md` 형식 사용 |
| **YAML 프론트매터** | `name`, `description`, `argument-hint` 필드로 스킬 메타데이터 정의 |
| **description 필드** | Claude가 자동 호출 시점을 판단할 수 있도록 구체적인 설명 작성 |
| **argument-hint** | `[기능명 또는 API 엔드포인트]` — `/` 자동완성 시 사용자에게 힌트 표시 |

**스킬에 포함된 내용:**
- 프로젝트 컨텍스트 (API 목록, 엔티티 관계, 비즈니스 규칙)
- 4단계 작업 절차 (시나리오 도출 → Gherkin → 우선순위 → 코드)
- 기술 규칙 (RestAssured 패턴, 데이터 셋업 규칙, 검증 원칙)

**핵심:** 범용 프롬프트를 스킬로 전환하면, 이후 `/acceptance-test 선물하기 기능`처럼 한 줄로 동일한 품질의 결과를 반복 생성할 수 있다. 공식 문서의 프론트매터를 활용하면 Claude가 "인수 테스트 작성해줘" 같은 자연어 요청에서도 스킬을 자동으로 인식하고 로드한다.

---

### 5단계: CLAUDE.md에 스킬 연동

**프롬프트:**
> 인수 테스트를 작성하는 게 최종 목표야. 스킬을 추가했어. 이를 CLAUDE.md에 반영해줘.

CLAUDE.md에 "인수 테스트 작성 가이드" 섹션을 추가하여, 인수 테스트 요청이 들어오면 반드시 스킬 절차를 따르도록 명시했다. 테스트 원칙(행동 기반, API 경계 검증, 실패 시나리오 증명)도 함께 기록했다.

이후 [Claude Code 메모리 공식 문서](https://code.claude.com/docs/ko/memory)를 참고하여 CLAUDE.md를 다듬었다.

**공식 문서 기반 적용 사항:**

| 항목 | 설명 |
|------|------|
| **메모리 계층 구조** | CLAUDE.md는 "프로젝트 메모리"로, 매 대화마다 자동 로드되어 팀 공유 지침 역할을 한다 |
| **구체적으로 작성** | "코드를 적절히 포맷합니다" 대신 "생성자 주입만 사용 — 필드 주입 금지, 파라미터에 `final`" 같은 구체적 규칙 |
| **구조를 사용하여 구성** | 글머리 기호로 개별 규칙 작성, 관련 규칙은 설명적인 마크다운 제목 아래 그룹화 |
| **스킬 경로 참조** | `.claude/skills/acceptance-test/SKILL.md` 경로를 명시하여 스킬과 메모리를 연결 |

**핵심:** 스킬과 CLAUDE.md를 연결하면, 스킬을 직접 호출하지 않더라도 "인수 테스트 작성해줘" 같은 자연어 요청에서도 동일한 절차가 적용된다. 공식 문서의 메모리 계층 구조를 이해하면, 어떤 지침을 어디에 둘지 판단할 수 있다.

---

### 6단계: 작성 순서 결정

**프롬프트:**
> 행위 하나씩 인수 테스트 작성할 건데, 어떤 행위부터 하는 게 좋을까?

Claude가 의존성 그래프를 분석하여 순서를 제안했다:

```
1. 카테고리 (의존성 없음, 가장 단순)
      ↓
2. 상품 (카테고리에 의존)
      ↓
3. 선물하기 (상품 + 옵션 + 회원에 의존, 핵심 비즈니스)
```

**근거:** 간단한 것부터 작성하면 스킬의 출력 품질을 검증하면서 점진적으로 복잡도를 높일 수 있다. 카테고리 테스트에서 스킬을 검증하고, 그 피드백을 상품과 선물하기에 반영했다.

---

### 7단계: 첫 번째 테스트 생성 (카테고리)

```
/acceptance-test 카테고리 등록
```

스킬이 4단계 절차(시나리오 → Gherkin → 우선순위 → 코드)를 따라 `CategoryAcceptanceTest`를 생성했다.

---

### 8단계: GPT로 생성 코드 분석

**GPT에게 한 요청:**
> createCategory 테스트 코드에 대해서 분석해줘.

**GPT의 역할:** 코드 리뷰어. Claude가 생성한 코드를 GPT가 검토하여 개선점을 도출했다.

**교훈:** 생성한 AI와 다른 AI로 검토하면, 한쪽의 편향을 다른 쪽이 잡아낼 수 있다. 이를 통해 스킬 규칙의 부족한 부분을 발견했다.

---

### 9단계: 스킬 규칙 보강

8단계의 분석 결과를 바탕으로 스킬에 세 가지 핵심 규칙을 추가했다:

| 규칙 | 설명 |
|------|------|
| **RestAssured 사용** | `given().when().then()` 패턴으로 HTTP 요청·검증 통일 |
| **API 경계에서만 검증** | Repository/Service 직접 조회 금지. HTTP 응답만으로 검증 |
| **실패 시나리오로 상태 변화 증명** | 재고 차감 같은 부수효과를 후속 요청의 실패로 간접 증명 |

**핵심:** 스킬은 한 번 만들고 끝이 아니라, 피드백을 반영하며 계속 개선한다. 이것이 단발성 프롬프트와 스킬의 차이다.

---

### 10단계: 피드백 반영하여 테스트 수정

**프롬프트:**
> CategoryAcceptanceTest, 스킬 반영해서 수정해줘.

보강된 스킬 규칙에 따라 기존 테스트를 수정했다. 이전 버전과의 차이:
- RestAssured `given().when().then()` 패턴 적용
- 응답 바디 필드 검증 추가 (`id`, `name`)
- 헬퍼 메서드 한국어 이름 적용

---

### 11단계: 테스트 코드 분석 및 스킬 동기화

**프롬프트:**
> 생성된 테스트 코드 분석하고 더 좋은 방식 있으면 알려주고 수정하며 스킬에도 반영해줘.

Claude가 테스트 코드를 자체 분석하고, 개선 사항을 코드와 스킬 양쪽에 반영했다. 이 단계에서 **코드 ↔ 스킬 양방향 동기화**가 이루어졌다.

```
코드 작성 → 코드 분석 → 개선 발견 → 코드 수정 + 스킬 수정
                                          ↑
                                     양방향 동기화
```

---

### 12단계: 나머지 테스트 일괄 작성

**프롬프트:**
> 나머지 행위에 대해서도 인수 테스트 코드 작성해줘.

6단계에서 정한 순서대로 `ProductAcceptanceTest`, `GiftAcceptanceTest`를 작성했다. 이전 단계에서 보강된 스킬이 적용되어, 추가 수정 없이 일관된 품질의 테스트가 생성되었다.

---

### 13단계: 테스트 전략 문서화

**프롬프트:**
> 테스트 코드, 스킬, CLAUDE.md를 참고해서 테스트 전략 문서를 작성해줘.

최종 산출물인 `TEST_STRATEGY.md`를 생성했다. 검증할 행위 목록, 데이터 전략, 검증 전략, 주요 의사결정을 정리하여 테스트의 "왜"를 문서화했다.

---

## AI 도구 역할 분담

이 프로젝트에서 Claude Code와 ChatGPT는 서로 다른 역할을 수행했다.

```
┌─────────────────────────────────────────────────────────────┐
│                                                             │
│   ChatGPT                          Claude Code              │
│   ────────                         ───────────              │
│   프롬프트 설계 (메타 프롬프팅)     프로젝트 분석            │
│   생성 코드 리뷰                    스킬 생성·수정           │
│   "어떻게 물어볼지" 설계            코드 생성·수정           │
│                                     빌드·테스트 실행         │
│                                     문서 생성                │
│                                                             │
│   ▸ 설계자 / 리뷰어                ▸ 실행자 / 빌더          │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

| 관점 | ChatGPT | Claude Code |
|------|---------|-------------|
| 강점 | 추상적 사고, 프롬프트 구조화, 비교 분석 | 코드베이스 직접 접근, 파일 생성·수정, 빌드 실행 |
| 활용 | "어떻게 시킬지" 고민할 때 | "실제로 실행할 때" |
| 빈도 | 3회 (3, 8단계 + 프롬프트 보조) | 11회 (나머지 전체 단계) |

---

## 핵심 교훈

### 1. 단순 프롬프트 → 구조화된 스킬로 진화시켜라

```
"분석해줘" (2단계)  →  프롬프트 템플릿 (3단계)  →  스킬 파일 (4단계)
     낮은 품질              중간 품질                 재현 가능한 고품질
```

한 번 쓰고 버리는 프롬프트 대신, 검증된 지시를 스킬로 저장하면 동일한 품질을 반복 생성할 수 있다.

### 2. 생성과 검토를 분리하라

같은 AI에게 생성과 검토를 모두 맡기면 자기 확증 편향이 생긴다. Claude가 생성한 코드를 GPT가 검토하고, 그 피드백을 Claude의 스킬에 반영하는 사이클이 효과적이었다.

### 3. CLAUDE.md와 스킬을 연결하라

스킬은 명시적으로 호출해야 작동하지만, CLAUDE.md에 "이 요청이 오면 이 스킬을 따르라"고 명시하면 자연어 요청에서도 동일한 절차가 적용된다. CLAUDE.md는 항상 로드되므로 프로젝트 전체의 행동 규칙이 된다.

### 4. 스킬은 점진적으로 개선하라

7단계(첫 생성) → 8단계(외부 리뷰) → 9단계(규칙 보강) → 10단계(재생성) → 11단계(자체 분석·보강)의 사이클을 거치며 스킬 품질이 수렴했다. 처음부터 완벽한 스킬을 만들려 하지 않고, 실제 출력을 보면서 개선했다.

### 5. 작성 순서가 품질을 결정한다

의존성이 적은 행위(카테고리)부터 시작하면 스킬 자체를 검증하는 기회가 된다. 간단한 케이스에서 스킬을 다듬고, 복잡한 케이스(선물하기)에서는 다듬어진 스킬이 높은 품질을 보장한다.
